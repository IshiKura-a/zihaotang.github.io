---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: home
---
Hi there, my name is **Zihao Tang**. Welcome to my website!

I am currently a student in [Zhejiang University](https://www.zju.edu.cn), working on my Master Degree advised by A.P. [Kun Kuang](https://kunkuang.github.io) and Prof. [Fei Wu](https://person.zju.edu.cn/wufei). I am also engaged in an internship at [MSRA](https://www.msra.cn/).

## Research Interests
I used to work in filed of **Model Compression** (KD (Knowledge Distillation), DFKD (Data Free Knowledge Distillation), OOD-KD (Out-of-Domain Knowledge Distillation) etc.) and **Domain Adaptation**.  
Currently, I have paid more attention to **Large-small-model Collaboration** and **LLM** (Large Language Model).

## Publications

**`[Arxiv]`** <span style="background:yellow;color:black">ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation</span>  
-- **Zihao Tang**, Zheqi Lv, Shengyu Zhang, Fei Wu, Kun Kuang.  
-- [**Paper**](https://arxiv.org/abs/2402.12408) \| [**Code**](https://github.com/IshiKura-a/ModelGPT)
> <span style="color:red;font-weight:bold;">Main Idea:</span>  
> User Requirement (data / description)  ---[<span style="color:red;font-weight:bold;">ModelGPT's inference</span>]--->  Off-the-Shelf Model!

**`[ICLR'24]`** AuG-KD: Anchor-Based Mixup Generation for Out-of-Domain Knowledge Distillation  
-- **Zihao Tang**, Zheqi Lv, Shengyu Zhang\*, Yifan Zhou, Xinyu Duan, Fei Wu, Kun Kuang\*.  
-- [**Paper**](https://arxiv.org/abs/2403.07030) \| [**Code**](https://github.com/IshiKura-a/AuG-KD)