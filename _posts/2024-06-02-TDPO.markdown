---
layout: post
title:  "Token-level Direct Preference Optimization"
date:   2024-06-02 12:00:00 +0800
categories: paper_reading
---

## **`[ICLM'24]`**

Finetuning pretrained LLMs is essential to align them with human values and intentions.
The overall process is often done with pairwise comparisons and KL divergence against a reference LLM, focusing on the evaluation of full answers generated by the models.
However, in contrast to the process of evaluation, the generation of answer occurs in a token level, following a sequential and auto-regressive fashion.

To more align the evaluation with the generation, this work propose **Token-level Direct Preference (TDPO)** to align LLMs with human preferences by optimizing policy at the token level.
TDPO incorporates forward KL divergence constraints for each token, improving alignment and diversity.
Utilizing the Bradley-Terry model for a token-based reward system, TDPO enhances the regulation of KL divergence, while preserving simplicity without the need for explicit reward modeling.

## Preliminaries

For language generation, a LM is prompted with prompt $x$ to generate a response $y$, both of which consist a sequence of tokens.
Direct Preference Optimization (DPO) commences with the RL objective from the RLHF:

$$
\max_{\pi_\theta}\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_\theta(\cdot|x)}\big\[r(x,y)-\beta\cdot \rm{KL}(\pi_\theta(\cdot|x)\parallel\pi_{\rm ref}(\cdot|x))\big\],
$$

where $\mathcal{D}$ represents the human preference dataset, $r(x,y)$ denotes the reward function, $\pi_{\rm ref}(\cdot\|x)$ serves as a reference model (typically chosen the LM after SFT), $\pi_\theta(\cdot\|x)$ represents the model undergoing RL finetuing, initialized with $\pi_\theta=\pi_{\rm ref}$.

DPO established a mapping between the reward model and the optimal policy under the **reverse KL divergence**:

$$
r(x,y)=\beta\cdot\log\frac{\pi_\theta(y|x)}{\pi_{\rm ref}(y|x)}+\beta\cdot\log Z(x),
$$
where $Z(x)$ is the partition function.

To align with human preference, DPO uses the Bradley-Terry model for pairwise comparisons:

$$
P_{BT}(y_1\succ y_2|x)=\frac{\exp(r(x,y_1))}{\exp(r(x,y_1))+\exp(r(x,y_2))}.
$$

Hence, DPO derives the objective function:

$$
u(x,y_w,y_l)=\beta\cdot\log\frac{\pi_\theta(y_w|x)}{\pi_{\rm ref}(y_w|x)}-\beta\cdot\log\frac{\pi_\theta(y_l|x)}{\pi_{\rm ref}(y_l|x)},
$$

$$
L_{DPO}=-\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}[\log\sigma(u(x,y_w,y_l))]
$$

## Methodology
Let's denote the response consists of $T$ tokens $y$ as $y^{<T+1}:=[y_1,\cdots,y_T]$.
When modeling text generation as a Markov decision process a state is a combination of the prompt and the generated response up to the current step, denoted as $s_t:=[x,y^{<t}]$.
An action corresponds to the next generated token, denoted as $a_t:=y^t$.
The token-wise reward is defined as $R_t:=R(s_t,a_t)$.

Expanding on the provided definitions, we establish the state-action function $Q_\pi$, the state value function $V_\pi$ and the advantage function $A_\pi$ for a policy $\pi$:

$$
Q_\pi(s_t,a_t)=\mathbb{E}_\pi\big\[\sum_{k=0}^{\infty}{\gamma^kR_{t+k}|s_t,a_t}\big\],
$$

$$
V_\pi(s_t)=\mathbb{E}_\pi\big\[Q_\pi(s_t,a_t)|s_t\big\],
$$

$$
A_\pi(s_t,a_t)=Q_\pi(s_t,a_t)-V_\pi(s_t).
$$

In contrast to DPO's sentence-level objective, this work proposes a token-level objective:

$$
\max_{\pi_\theta}\mathbb{E}\big\[A_{\pi_{\rm ref}}(s_t,z)-\beta\cdot{KL}(\pi_\theta(\cdot\|s_t)\parallel\pi_{\rm ref}(\cdot\|s_t))\big\].
$$

This constrained problem has closed-form solution:

$$
\pi^*_\theta(z|s_t)=\frac{\pi_{\rm ref}(z|s_t)\cdot\exp(\frac{1}{\beta}\cdot Q_{\pi_{\rm ref}}(s_t,z))}{Z(s_t;\beta)},
$$

$$
Z(s_t;\beta)=\mathbb{E}_{z\sim\pi\_{\rm ref}(\cdot\|s_t)}[\exp(\frac{1}{\beta}\cdot Q\_{\pi\_{\rm ref}}(s_t,z))],
$$

where $Z$ is the partition function.

To facilitate subsequent derivations, this work first introduce the sequential KL divergence:

$$
{\rm SeqKL}(x,y;\pi_1\parallel\pi_2)=\sum_{t=1}^T KL(\pi_1(\cdot;s_t)\parallel\pi_2(\cdot;s_t)).
$$

In the KL-constrained advantage function maximization problem, he Bradley-Terry model express the human preference probability in terms of the optimal policy $\pi^*\_\theta$ and reference policy $\pi\_{\rm ref}$:

$$
P^*_{BT}(y_1\succ y_2|x)=\sigma(u^*(x,y_1,y_2)-\delta^*(x,y_1,y_2)),
$$
where $u$ is the difference in rewards implicitly declared above, and $\delta$ is the difference in sequential forward KL divergence:

$$
\delta(x,y_1,y_2)=\beta\cdot{\rm SeqKL}(x,y_2;\pi\_{\rm ref}\parallel\pi\_\theta)-\beta\cdot{\rm SeqKL}(x,y_1;\pi\_{\rm ref}\parallel\pi\_\theta).
$$

Consequently, the initial version of TDPO can be represented as:
$$
L_{TDPO\_1}(\pi_\theta;\pi\_{\rm ref})=-\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}[\log\sigma(u(x,y_w,y_l)-\delta(x,y_w,y_l))].
$$

For improved performance, we could stop the gradient of the loss to obtain the second version of TDPO. We put the summarization of the loss function below:
![fig/loss](/assets/240602000.png)