<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>SimPO: Simple Preference Optimization with a Reference-Free Reward | Zihao Tang’s Homepage</title>
<meta name="generator" content="Jekyll v3.9.5">
<meta property="og:title" content="SimPO: Simple Preference Optimization with a Reference-Free Reward">
<meta property="og:locale" content="en_US">
<meta name="description" content="For background about DPO, readers are kindly referred to TDPO.">
<meta property="og:description" content="For background about DPO, readers are kindly referred to TDPO.">
<link rel="canonical" href="https://ishikura-a.github.io/paper_reading/2024/06/02/SimPO.html">
<meta property="og:url" content="https://ishikura-a.github.io/paper_reading/2024/06/02/SimPO.html">
<meta property="og:site_name" content="Zihao Tang’s Homepage">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2024-06-02T04:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="SimPO: Simple Preference Optimization with a Reference-Free Reward">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-06-02T04:00:00+00:00","datePublished":"2024-06-02T04:00:00+00:00","description":"For background about DPO, readers are kindly referred to TDPO.","headline":"SimPO: Simple Preference Optimization with a Reference-Free Reward","mainEntityOfPage":{"@type":"WebPage","@id":"https://ishikura-a.github.io/paper_reading/2024/06/02/SimPO.html"},"url":"https://ishikura-a.github.io/paper_reading/2024/06/02/SimPO.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css">
<link type="application/atom+xml" rel="alternate" href="https://ishikura-a.github.io/feed.xml" title="Zihao Tang's Homepage">
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<header class="site-header" role="banner">

  <div class="wrapper">
<a class="site-title" rel="author" href="/">Zihao Tang's Homepage</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">SimPO: Simple Preference Optimization with a Reference-Free Reward</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-06-02T04:00:00+00:00" itemprop="datePublished">Jun 2, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>For background about DPO, readers are kindly referred to <a href="https://ishikura-a.github.io/paper_reading/2024/06/02/TDPO.html">TDPO</a>.</p>

<p>Drawbacks of DPO:</p>
<ol>
  <li>need a reference model</li>
  <li>the reward formulation is not directly aligned with the metric used to guide generation</li>
</ol>

<p>In response, this work propose SimPO, a simple yet effective offline preference optimization algorithm.
The core of the algorithm aligns the reward function in the preference optimization objective
with the generation metric. SimPO consists of two major components:</p>
<ol>
  <li>
<strong>a length-normalized reward</strong>, calculated as the average log probability of all tokens in a response using the policy model.</li>
  <li>
<strong>a target reward margin</strong> to ensure the reward difference between winning and losing responses exceeds this margin.</li>
</ol>

<p>Adopting the reward function defined in DPO as the following drawbacks:</p>
<ol>
  <li>the requirement of a reference model $\pi_{\rm ref}$ during training incurs additional memory and computational costs.</li>
  <li>there is a discrepancy between the reward being optimized during training and the generation metric used for inference</li>
</ol>

<p>During generation, the policy model $\pi_\theta$ is used to generate a sequence that approximately maximizes the average log likelihood, defined as follows:</p>

<p>$$
p_\theta(y|x)=\frac{1}{|y|}\log\pi_\theta(y|x)=\frac{1}{|y|}\sum_{i=1}^{|y|}\log\pi_\theta(y_i|x,y_{&lt;i}).
$$</p>

<p>While direct maximization of this metric during decoding is intractable, in DPO, for any triple $(x,y_w,y_l)$, satisfying the reward ranking $r(x,y_w) &gt; r(x,y_l)$ does not necessarily mean that the likelihood ranking $p_\theta(y_w|x) &gt; p_\theta(y_l|x)$ is met.
It is natural to replace the reward function in DPO with $p_\theta$ since it aligns with the likelihood metric that guides generation, resulting in a length-normalized reward:</p>

<p>$$
r_{\rm SimPO}(x,y)=\beta\cdot p_\theta(y|x),
$$
where $\beta$ is a constant that controls the scaling of the reward difference.</p>

<p>Additionally, they introduce a target reward margin term $\gamma&gt;0$, to the Bradley-Terry objective to ensure that the reward for the winning response $r(x,y_w)$, exceeds the reward for the losing response $r(x,y_l)$ by at least $\gamma$:</p>

<p>$$
p(y_w\succ y_l|x)=\sigma(r(x,y_w)-r(x,y_l)-\gamma).
$$</p>

<p>Hence, we can derive the objective function of SimPO by NLL loss:
$$
L_{\rm SimPO}(\pi_\theta)=-\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}[\log\sigma(p(y_w\succ y_l|x))].
$$</p>

<p>The comparison of objective functions between SimPO and other baselines is put in the table below:
<img src="/assets/240602001.png" alt="comp"></p>

  </div>
<a class="u-url" href="/paper_reading/2024/06/02/SimPO.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Zihao Tang's Homepage</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Zihao Tang's Homepage</li>
<li><a class="u-email" href="mailto:tangzihao@zju.edu.cn">tangzihao@zju.edu.cn</a></li>
</ul>
      </div>

      <div class="footer-col footer-col-2">
<ul class="social-media-list"><li><a href="https://github.com/Ishikura-a"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">Ishikura-a</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>There should be something, but I just haven't figured it out :).</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
